{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPENdLLWVFE_",
    "outputId": "4ebccf38-3a84-4a32-b9bf-8c7e8afe1db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (3.12)\n",
      "Requirement already satisfied: h5py in /home/cpslab/.local/lib/python3.6/site-packages (2.10.0)\n",
      "Requirement already satisfied: six in /home/cpslab/.local/lib/python3.6/site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/cpslab/.local/lib/python3.6/site-packages (from h5py) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H-fmr9bNKuTj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYbDmV48K8tF",
    "outputId": "00573df6-a853-456a-cc5a-679b10192157"
   },
   "outputs": [],
   "source": [
    "# os.listdir(\"./finished/train/dataraw/hires\") #listing the contents in the location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KVdg12n3LC48"
   },
   "outputs": [],
   "source": [
    "file_path=\"./finished/train/dataraw/hires/\" #path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eIeycOOZLWqn"
   },
   "outputs": [],
   "source": [
    "def create_lr_data(img_path=\"\",img_file=\"\",compression_factor=10,w=0,h=0):\n",
    "  \n",
    "  img=cv2.imread(img_path+img_file)\n",
    "  if w!=0 and h!=0:\n",
    "    newimg1=cv2.resize(img,(w,h),interpolation=cv2.INTER_CUBIC)\n",
    "    # cv2_imshow(newimg1)\n",
    "    return newimg1\n",
    "  else:\n",
    "    w=img.shape[1]  \n",
    "    h=img.shape[0]  \n",
    "    newimg1=cv2.resize(img,(int(w/compression_factor),int(h/compression_factor)),interpolation=cv2.INTER_CUBIC)\n",
    "    # cv2_imshow(newimg1)\n",
    "    return newimg1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DoqP6kDCMBcu"
   },
   "outputs": [],
   "source": [
    "hr_arr=[]\n",
    "lr_arr=[]\n",
    "error_images=[]\n",
    "for imgs in os.listdir(file_path):\n",
    "  try:\n",
    "    img_hr=create_lr_data(img_path=file_path, img_file=imgs, w=800,h=800)\n",
    "    hr_arr.append(img_hr)\n",
    "    img_lr=create_lr_data(img_path=file_path, img_file=imgs, w=200,h=200)\n",
    "    lr_arr.append(img_lr)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    error_images.append(imgs)\n",
    "  # cv2_imshow(img_lr)\n",
    "  # break\n",
    "hr_arr=np.array(hr_arr)\n",
    "lr_arr=np.array(lr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BYkXzDmZMeR-",
    "outputId": "90ce484a-b99e-40c8-ad23-4bdd8c614657"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNTWkwyDeoSu"
   },
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g_v1diDHecDd",
    "outputId": "ca4c5162-fe10-445e-b7be-58da6ca33a7c"
   },
   "outputs": [],
   "source": [
    "###MODEL\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "inp= tf.keras.Input(shape=(200, 200, 3))\n",
    "#1st convolution\n",
    "conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv_1')(inp)\n",
    "\n",
    "#2nd convolution\n",
    "conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv_2')(conv1)\n",
    "\n",
    "#3rd convolution\n",
    "conv3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv_3')(conv2)\n",
    "\n",
    "#skip connection between the 1st and 3rd convolution\n",
    "skip = tf.keras.layers.Add()([conv1,conv3])\n",
    "\n",
    "\n",
    "#bottleneck\n",
    "bot = tf.keras.layers.Conv2D(64, (1,1), activation='relu')(skip)\n",
    "\n",
    "#upsampling layer\n",
    "\n",
    "up= tf.keras.layers.Conv2DTranspose(64, (2,2),strides=(4,4), padding='same', input_shape=(200, 200, 64))(bot)\n",
    "#bottleneck\n",
    "x = tf.keras.layers.Conv2D(4, (1,1), activation='relu')(up)\n",
    "\n",
    "\n",
    "\n",
    "#RECONSTUCTION STAGE ONE\n",
    "\n",
    "#first block\n",
    "\n",
    "#Asymmetric convolution 1\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Asymmetric convolution 2\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Asymmetric convolution 3\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "                           \n",
    "#Asymmetric convolution 4\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Residual connection\n",
    "x = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)  # Residual Connection\n",
    "\n",
    "x = tf.keras.layers.Add()([x, y])  # Addition\n",
    "\n",
    "#second block\n",
    "\n",
    "#Asymmetric convolution 1\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Asymmetric convolution 2\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Asymmetric convolution 3\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "                           \n",
    "#Asymmetric convolution 4\n",
    "y = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(y)\n",
    "\n",
    "y = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(y)\n",
    "\n",
    "#Residual connection\n",
    "x = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)  # Residual Connection\n",
    "\n",
    "x = tf.keras.layers.Add()([x, y])  # Addition\n",
    "\n",
    "\n",
    "#RECONSTUCTION STAGE TWO\n",
    "#INCEPTION BLOCK OF 5 TOWERS\n",
    "                           \n",
    "T11 = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)\n",
    "\n",
    "T12 = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(T11)\n",
    "                           \n",
    "T13 = tf.keras.layers.Conv2D(16, (5, 1), padding='same')(T12)\n",
    "\n",
    "T1 = tf.keras.layers.Conv2D(16, (1, 5), activation='relu', padding='same')(T13)\n",
    "                           \n",
    "                           \n",
    "T21 = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)\n",
    "\n",
    "T22 = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(T21)\n",
    "                           \n",
    "T23 = tf.keras.layers.Conv2D(16, (5, 1), padding='same')(T22)\n",
    "\n",
    "T2 = tf.keras.layers.Conv2D(16, (1, 5), activation='relu', padding='same')(T23)\n",
    "                           \n",
    "\n",
    "T31 = tf.keras.layers.Conv2D(16, (3, 1), padding='same')(x)\n",
    "\n",
    "T32 = tf.keras.layers.Conv2D(16, (1, 3), activation='relu', padding='same')(T31)\n",
    "                           \n",
    "T33 = tf.keras.layers.Conv2D(16, (5, 1), padding='same')(T32)\n",
    "\n",
    "T3 = tf.keras.layers.Conv2D(16, (1, 5), activation='relu', padding='same')(T33)  \n",
    "                           \n",
    "                           \n",
    "T41 = tf.keras.layers.Conv2D(16, (7, 1), padding='same')(x)\n",
    "\n",
    "T42 = tf.keras.layers.Conv2D(16, (1, 7), activation='relu', padding='same')(T41)\n",
    "                           \n",
    "T43 = tf.keras.layers.Conv2D(16, (9, 1), padding='same')(T42)\n",
    "\n",
    "T4 = tf.keras.layers.Conv2D(16, (1, 9), activation='relu', padding='same')(T43)  \n",
    "                           \n",
    "                           \n",
    "T51 = tf.keras.layers.Conv2D(16, (7, 1), padding='same')(x)\n",
    "\n",
    "T52 = tf.keras.layers.Conv2D(16, (1, 7), activation='relu', padding='same')(T51)\n",
    "                           \n",
    "T53 = tf.keras.layers.Conv2D(16, (9, 1), padding='same')(T52)\n",
    "\n",
    "T5 = tf.keras.layers.Conv2D(16, (1, 9), activation='relu', padding='same')(T53)  \n",
    "                           \n",
    "                           \n",
    "\n",
    "\n",
    "#CONCATENATION OF OUTPUT OF FIVE TOWERS\n",
    "                 \n",
    "T= tf.keras.layers.Concatenate(axis=3)([T1,T2,T3,T4,T5])\n",
    "\n",
    "\n",
    "#BOTTLENECK\n",
    "y_pred = tf.keras.layers.Conv2D(3, (1,1), activation='relu',)(T)\n",
    "\n",
    "model = tf.keras.Model(inp,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTDvJEZaety3"
   },
   "source": [
    "## **Model Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2llqdlaer52",
    "outputId": "bc2049ad-a48f-412f-95f6-2033f5998842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 200, 200, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 200, 200, 64) 36928       conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 200, 200, 64) 36928       conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 200, 200, 64) 0           conv_1[0][0]                     \n",
      "                                                                 conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 200, 200, 64) 4160        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 800, 800, 64) 16448       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 800, 800, 4)  260         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 800, 800, 16) 208         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 800, 800, 16) 784         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 800, 800, 16) 784         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 800, 800, 16) 784         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 800, 800, 16) 784         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 800, 800, 16) 784         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 800, 800, 16) 784         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 800, 800, 16) 208         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 800, 800, 16) 784         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 800, 800, 16) 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 800, 800, 16) 784         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 800, 800, 16) 784         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 800, 800, 16) 784         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 800, 800, 16) 784         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 800, 800, 16) 784         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 800, 800, 16) 784         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 800, 800, 16) 784         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 800, 800, 16) 784         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 800, 800, 16) 784         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 800, 800, 16) 0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 800, 800, 16) 784         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 800, 800, 16) 784         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 800, 800, 16) 784         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 800, 800, 16) 1808        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 800, 800, 16) 1808        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 800, 800, 16) 784         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 800, 800, 16) 784         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 800, 800, 16) 784         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 800, 800, 16) 1808        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 800, 800, 16) 1808        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 800, 800, 16) 2320        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 800, 800, 16) 2320        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 800, 800, 16) 1296        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 800, 800, 16) 2320        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 800, 800, 16) 2320        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 800, 800, 80) 0           conv2d_23[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 800, 800, 3)  243         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 138,711\n",
      "Trainable params: 138,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y-1Yvt-Z1GG2"
   },
   "outputs": [],
   "source": [
    "Adam1 = keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GH-pWBbmtoNo"
   },
   "outputs": [],
   "source": [
    "model.compile(loss ='mean_squared_error', optimizer=Adam1, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H31Wqeyhtqmm"
   },
   "source": [
    "# **Training using Tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZCDEcTWftHf-"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YxT_4argtLP0",
    "outputId": "c4fea696-986c-429d-9382-9a38cbc66850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Frn_twcZtOlE"
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs_150/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YEn0aPG7tdwt"
   },
   "outputs": [],
   "source": [
    "my_callbacks = [tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFfy7461teww",
    "outputId": "8c214a56-0da5-47b4-e872-db78305f02cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4128 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpslab/ml2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4128/4128 [==============================] - 760s 184ms/sample - loss: 486.0151 - acc: 0.7402\n",
      "Epoch 2/200\n",
      "4128/4128 [==============================] - 692s 168ms/sample - loss: 254.3669 - acc: 0.8490\n",
      "Epoch 3/200\n",
      "4128/4128 [==============================] - 692s 168ms/sample - loss: 243.3304 - acc: 0.8586\n",
      "Epoch 4/200\n",
      "4128/4128 [==============================] - 693s 168ms/sample - loss: 235.8107 - acc: 0.8733\n",
      "Epoch 5/200\n",
      "4128/4128 [==============================] - 689s 167ms/sample - loss: 233.6769 - acc: 0.8749\n",
      "Epoch 6/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 229.2638 - acc: 0.8784\n",
      "Epoch 7/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 226.1828 - acc: 0.8791\n",
      "Epoch 8/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 225.9040 - acc: 0.8787\n",
      "Epoch 9/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 223.7651 - acc: 0.8810\n",
      "Epoch 10/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 221.4238 - acc: 0.8912\n",
      "Epoch 11/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 221.4521 - acc: 0.8840\n",
      "Epoch 12/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 221.0903 - acc: 0.8869\n",
      "Epoch 13/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 218.5069 - acc: 0.8907\n",
      "Epoch 14/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 218.4922 - acc: 0.8896\n",
      "Epoch 15/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 218.5039 - acc: 0.8921\n",
      "Epoch 16/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 217.5067 - acc: 0.8925\n",
      "Epoch 17/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 216.2620 - acc: 0.8950\n",
      "Epoch 18/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 216.0994 - acc: 0.8938\n",
      "Epoch 19/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 215.9358 - acc: 0.8910\n",
      "Epoch 20/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 214.9323 - acc: 0.8918\n",
      "Epoch 21/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 215.4998 - acc: 0.8951\n",
      "Epoch 22/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 214.3236 - acc: 0.8961\n",
      "Epoch 23/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 213.9634 - acc: 0.8973\n",
      "Epoch 24/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 214.5813 - acc: 0.8969\n",
      "Epoch 25/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 212.5287 - acc: 0.8985\n",
      "Epoch 26/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 212.9136 - acc: 0.8975\n",
      "Epoch 27/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 212.8306 - acc: 0.9006\n",
      "Epoch 28/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 212.1371 - acc: 0.8985\n",
      "Epoch 29/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 212.0469 - acc: 0.8976\n",
      "Epoch 30/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 211.4815 - acc: 0.9019\n",
      "Epoch 31/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 211.8793 - acc: 0.9019\n",
      "Epoch 32/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 211.3782 - acc: 0.8989\n",
      "Epoch 33/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 211.0743 - acc: 0.9008\n",
      "Epoch 34/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 210.3857 - acc: 0.9023\n",
      "Epoch 35/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 211.0162 - acc: 0.9030\n",
      "Epoch 36/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 210.0951 - acc: 0.9012\n",
      "Epoch 37/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 210.0491 - acc: 0.9021\n",
      "Epoch 38/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 210.3937 - acc: 0.9019\n",
      "Epoch 39/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 209.9700 - acc: 0.9018\n",
      "Epoch 40/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 209.2324 - acc: 0.9045\n",
      "Epoch 41/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 209.4131 - acc: 0.9031\n",
      "Epoch 42/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 209.8671 - acc: 0.8988\n",
      "Epoch 43/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 209.1980 - acc: 0.9024\n",
      "Epoch 44/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 209.1157 - acc: 0.9022\n",
      "Epoch 45/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.6716 - acc: 0.9026\n",
      "Epoch 46/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 208.6838 - acc: 0.9044\n",
      "Epoch 47/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.3389 - acc: 0.9042\n",
      "Epoch 48/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.8974 - acc: 0.9064\n",
      "Epoch 49/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.5057 - acc: 0.9033\n",
      "Epoch 50/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.5332 - acc: 0.9044\n",
      "Epoch 51/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.3006 - acc: 0.9038\n",
      "Epoch 52/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.1299 - acc: 0.9060\n",
      "Epoch 53/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 208.0246 - acc: 0.9063\n",
      "Epoch 54/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.6439 - acc: 0.9067\n",
      "Epoch 55/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.8201 - acc: 0.9056\n",
      "Epoch 56/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 207.5326 - acc: 0.9070\n",
      "Epoch 57/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.3830 - acc: 0.9052\n",
      "Epoch 58/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.2485 - acc: 0.9074\n",
      "Epoch 59/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.2754 - acc: 0.9055\n",
      "Epoch 60/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.1990 - acc: 0.9083\n",
      "Epoch 61/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.4537 - acc: 0.9055\n",
      "Epoch 62/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.8034 - acc: 0.9076\n",
      "Epoch 63/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.1928 - acc: 0.9082\n",
      "Epoch 64/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.7236 - acc: 0.9065\n",
      "Epoch 65/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.7511 - acc: 0.9078\n",
      "Epoch 66/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.4633 - acc: 0.9084\n",
      "Epoch 67/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.6212 - acc: 0.9093\n",
      "Epoch 68/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.6620 - acc: 0.9107\n",
      "Epoch 69/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.6346 - acc: 0.9082\n",
      "Epoch 70/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.2620 - acc: 0.9085\n",
      "Epoch 71/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.6348 - acc: 0.9076\n",
      "Epoch 72/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.2485 - acc: 0.9084\n",
      "Epoch 73/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.0623 - acc: 0.9098\n",
      "Epoch 74/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 206.0536 - acc: 0.9097\n",
      "Epoch 75/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.7317 - acc: 0.9088\n",
      "Epoch 76/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.9624 - acc: 0.9081\n",
      "Epoch 77/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 207.0359 - acc: 0.9082\n",
      "Epoch 78/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.4434 - acc: 0.9104\n",
      "Epoch 79/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.6783 - acc: 0.9101\n",
      "Epoch 80/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.6854 - acc: 0.9081\n",
      "Epoch 81/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.6999 - acc: 0.9081\n",
      "Epoch 82/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.6631 - acc: 0.9082\n",
      "Epoch 83/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.3840 - acc: 0.9098\n",
      "Epoch 84/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 205.4446 - acc: 0.9100\n",
      "Epoch 85/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.4041 - acc: 0.9094\n",
      "Epoch 86/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.3302 - acc: 0.9092\n",
      "Epoch 87/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.2082 - acc: 0.9102\n",
      "Epoch 88/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.5938 - acc: 0.9100\n",
      "Epoch 89/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.8395 - acc: 0.9107\n",
      "Epoch 90/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.1737 - acc: 0.9081\n",
      "Epoch 91/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.0306 - acc: 0.9088\n",
      "Epoch 92/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.2402 - acc: 0.9094\n",
      "Epoch 93/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.6499 - acc: 0.9110\n",
      "Epoch 94/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.0257 - acc: 0.9079\n",
      "Epoch 95/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.9380 - acc: 0.9071\n",
      "Epoch 96/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 205.0037 - acc: 0.9137\n",
      "Epoch 97/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.5394 - acc: 0.9099\n",
      "Epoch 98/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.8193 - acc: 0.9080\n",
      "Epoch 99/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.5283 - acc: 0.9110\n",
      "Epoch 100/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.5155 - acc: 0.9089\n",
      "Epoch 101/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.4692 - acc: 0.9130\n",
      "Epoch 102/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.5035 - acc: 0.9099\n",
      "Epoch 103/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.2197 - acc: 0.9095\n",
      "Epoch 104/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.3448 - acc: 0.9108\n",
      "Epoch 105/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.1787 - acc: 0.9104\n",
      "Epoch 106/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.1959 - acc: 0.9115\n",
      "Epoch 107/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.1321 - acc: 0.9103\n",
      "Epoch 108/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.2922 - acc: 0.9094\n",
      "Epoch 109/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.2572 - acc: 0.9103\n",
      "Epoch 110/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.8631 - acc: 0.9116\n",
      "Epoch 111/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.8415 - acc: 0.9123\n",
      "Epoch 112/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.0242 - acc: 0.9117\n",
      "Epoch 113/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.8023 - acc: 0.9133\n",
      "Epoch 114/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.7646 - acc: 0.9116\n",
      "Epoch 115/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.8181 - acc: 0.9116\n",
      "Epoch 116/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.8460 - acc: 0.9119\n",
      "Epoch 117/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5594 - acc: 0.9134\n",
      "Epoch 118/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 204.1110 - acc: 0.9111\n",
      "Epoch 119/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5715 - acc: 0.9144\n",
      "Epoch 120/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5419 - acc: 0.9124\n",
      "Epoch 121/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.4509 - acc: 0.9139\n",
      "Epoch 122/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.6532 - acc: 0.9139\n",
      "Epoch 123/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5766 - acc: 0.9096\n",
      "Epoch 124/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5008 - acc: 0.9134\n",
      "Epoch 125/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.5329 - acc: 0.9144\n",
      "Epoch 126/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.2514 - acc: 0.9141\n",
      "Epoch 127/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 203.3922 - acc: 0.9142\n",
      "Epoch 128/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.2580 - acc: 0.9157\n",
      "Epoch 129/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.3160 - acc: 0.9121\n",
      "Epoch 130/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.9572 - acc: 0.9138\n",
      "Epoch 131/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.2706 - acc: 0.9118\n",
      "Epoch 132/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.0355 - acc: 0.9134\n",
      "Epoch 133/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.2924 - acc: 0.9144\n",
      "Epoch 134/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 203.1527 - acc: 0.9133\n",
      "Epoch 135/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 203.2372 - acc: 0.9153\n",
      "Epoch 136/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.9471 - acc: 0.9127\n",
      "Epoch 137/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 203.2357 - acc: 0.9147\n",
      "Epoch 138/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.8438 - acc: 0.9148\n",
      "Epoch 139/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.8246 - acc: 0.9140\n",
      "Epoch 140/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.9066 - acc: 0.9122\n",
      "Epoch 141/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.9580 - acc: 0.9140\n",
      "Epoch 142/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.6905 - acc: 0.9124\n",
      "Epoch 143/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.6697 - acc: 0.9147\n",
      "Epoch 144/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.9291 - acc: 0.9124\n",
      "Epoch 145/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.6097 - acc: 0.9149\n",
      "Epoch 146/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.7651 - acc: 0.9146\n",
      "Epoch 147/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.7047 - acc: 0.9150\n",
      "Epoch 148/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.5452 - acc: 0.9143\n",
      "Epoch 149/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.6865 - acc: 0.9145\n",
      "Epoch 150/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.6048 - acc: 0.9128\n",
      "Epoch 151/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.5651 - acc: 0.9148\n",
      "Epoch 152/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.5234 - acc: 0.9141\n",
      "Epoch 153/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.7794 - acc: 0.9159\n",
      "Epoch 154/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.5128 - acc: 0.9137\n",
      "Epoch 155/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.5198 - acc: 0.9145\n",
      "Epoch 156/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.5607 - acc: 0.9142\n",
      "Epoch 157/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.2158 - acc: 0.9147\n",
      "Epoch 158/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.5503 - acc: 0.9144\n",
      "Epoch 159/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.3165 - acc: 0.9144\n",
      "Epoch 160/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.1795 - acc: 0.9156\n",
      "Epoch 161/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.2443 - acc: 0.9134\n",
      "Epoch 162/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.1910 - acc: 0.9153\n",
      "Epoch 163/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.1563 - acc: 0.9138\n",
      "Epoch 164/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.2100 - acc: 0.9141\n",
      "Epoch 165/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.2129 - acc: 0.9149\n",
      "Epoch 166/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.9416 - acc: 0.9145\n",
      "Epoch 167/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.9417 - acc: 0.9152\n",
      "Epoch 168/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.0051 - acc: 0.9148\n",
      "Epoch 169/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.1369 - acc: 0.9147\n",
      "Epoch 170/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.9261 - acc: 0.9166\n",
      "Epoch 171/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.9062 - acc: 0.9136\n",
      "Epoch 172/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.8864 - acc: 0.9157\n",
      "Epoch 173/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.9728 - acc: 0.9145\n",
      "Epoch 174/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.8616 - acc: 0.9157\n",
      "Epoch 175/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 202.1030 - acc: 0.9149\n",
      "Epoch 176/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.6992 - acc: 0.9168\n",
      "Epoch 177/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.7584 - acc: 0.9132\n",
      "Epoch 178/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.7864 - acc: 0.9146\n",
      "Epoch 179/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.6193 - acc: 0.9148\n",
      "Epoch 180/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.8564 - acc: 0.9149\n",
      "Epoch 181/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.6027 - acc: 0.9167\n",
      "Epoch 182/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.6933 - acc: 0.9129\n",
      "Epoch 183/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.7674 - acc: 0.9164\n",
      "Epoch 184/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.4119 - acc: 0.9146\n",
      "Epoch 185/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.8192 - acc: 0.9138\n",
      "Epoch 186/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.3551 - acc: 0.9149\n",
      "Epoch 187/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 202.1441 - acc: 0.9163\n",
      "Epoch 188/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.4488 - acc: 0.9156\n",
      "Epoch 189/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.3191 - acc: 0.9165\n",
      "Epoch 190/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.4812 - acc: 0.9157\n",
      "Epoch 191/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.5331 - acc: 0.9162\n",
      "Epoch 192/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.4622 - acc: 0.9154\n",
      "Epoch 193/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.3660 - acc: 0.9156\n",
      "Epoch 194/200\n",
      "4128/4128 [==============================] - 687s 166ms/sample - loss: 201.2168 - acc: 0.9154\n",
      "Epoch 195/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.3257 - acc: 0.9144\n",
      "Epoch 196/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.5257 - acc: 0.9151\n",
      "Epoch 197/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.3164 - acc: 0.9167\n",
      "Epoch 198/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.5865 - acc: 0.9154\n",
      "Epoch 199/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.2996 - acc: 0.9156\n",
      "Epoch 200/200\n",
      "4128/4128 [==============================] - 686s 166ms/sample - loss: 201.4076 - acc: 0.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fada669cd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(lr_arr, hr_arr, epochs=200, batch_size=2, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0EKmIo_bR8jK"
   },
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model_200_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hhWvE2leR8jK"
   },
   "outputs": [],
   "source": [
    "!mkdir -p saved_model_h5\n",
    "model.save('my_h5_model_200_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cJvbwwEbe3B9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meYv3yb0uT8F"
   },
   "source": [
    "## Preprocessed and processed image outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqAGMIaPe8Cm"
   },
   "outputs": [],
   "source": [
    "img_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scHOPb2qTxsv"
   },
   "outputs": [],
   "source": [
    "cv2.imshow('image',img_lr.reshape(200,200,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEtqizrXnEoT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow('image',img_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7OuzAjET9vE"
   },
   "outputs": [],
   "source": [
    "img_lr=img_lr.reshape((1,)+img_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0C63o7aI8hw"
   },
   "outputs": [],
   "source": [
    "img_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-OM5NxsT-d-"
   },
   "outputs": [],
   "source": [
    "out=model.predict([img_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZntznirKDX0"
   },
   "source": [
    "# **SSIM** **CALCULATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ3_vrdXG7HS"
   },
   "outputs": [],
   "source": [
    "im1 = tf.image.convert_image_dtype(img_hr, tf.float32)\n",
    "im2 = tf.image.convert_image_dtype(out, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hetqCKsqGx82"
   },
   "outputs": [],
   "source": [
    "ssim2 = tf.image.ssim(im1, im2, max_val=1.0, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoyH2gzWJQvq"
   },
   "outputs": [],
   "source": [
    "print(ssim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml6WcQRSKTuF"
   },
   "source": [
    "# **OUTPUT IMAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUB6kgwyjn9Y"
   },
   "outputs": [],
   "source": [
    "cv2_imshow(out.reshape(800,800,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WIhOJoLnfHD"
   },
   "outputs": [],
   "source": [
    "model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VE0DPuPWpfXY"
   },
   "outputs": [],
   "source": [
    "dot_img_file = './drive/My Drive/model_1.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file, dpi=300,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9eSC4bo_XJfk"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs --host localhost --port 8088"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Trained_200.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
